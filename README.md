<div align="center">
<h1 style="border-bottom: none; margin-bottom: 0px ">Depth Anything 3: Recovering the Visual Space from Any Views</h1>
<!-- <h2 style="border-top: none; margin-top: 3px;">Recovering the Visual Space from Any Views</h2> -->


[**Haotong Lin**](https://haotongl.github.io/)<sup>&ast;</sup> Â· [**Sili Chen**](https://github.com/SiliChen321)<sup>&ast;</sup> Â· [**Jun Hao Liew**](https://liewjunhao.github.io/)<sup>&ast;</sup> Â· [**Donny Y. Chen**](https://donydchen.github.io)<sup>&ast;</sup> Â· [**Zhenyu Li**](https://zhyever.github.io/) Â· [**Guang Shi**](https://scholar.google.com/citations?user=MjXxWbUAAAAJ&hl=en) Â· [**Jiashi Feng**](https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en)
<br>
[**Bingyi Kang**](https://bingykang.github.io/)<sup>&ast;&dagger;</sup>

&dagger;project lead&emsp;&ast;Equal Contribution

<a href="https://arxiv.org/abs/2511.10647"><img src='https://img.shields.io/badge/arXiv-Depth Anything 3-red' alt='Paper PDF'></a>
<a href='https://depth-anything-3.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything 3-green' alt='Project Page'></a>
<a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-3'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>
<!-- <a href='https://huggingface.co/datasets/depth-anything/VGB'><img src='https://img.shields.io/badge/Benchmark-VisGeo-yellow' alt='Benchmark'></a> -->
<!-- <a href='https://huggingface.co/datasets/depth-anything/data'><img src='https://img.shields.io/badge/Benchmark-xxx-yellow' alt='Data'></a> -->

</div>

This work presents **Depth Anything 3 (DA3)**, a model that predicts spatially consistent geometry from
arbitrary visual inputs, with or without known camera poses.
In pursuit of minimal modeling, DA3 yields two key insights:
- ğŸ’ A **single plain transformer** (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization,
- âœ¨ A singular **depth-ray representation** obviates the need for complex multi-task learning.

ğŸ† DA3 significantly outperforms
[DA2](https://github.com/DepthAnything/Depth-Anything-V2) for monocular depth estimation,
and [VGGT](https://github.com/facebookresearch/vggt) for multi-view depth estimation and pose estimation.
All models are trained exclusively on **public academic datasets**.

<!-- <p align="center">
  <img src="assets/images/da3_teaser.png" alt="Depth Anything 3" width="100%">
</p> -->
<p align="center">
  <img src="assets/images/demo320-2.gif" alt="Depth Anything 3 - Left" width="70%">
</p>
<p align="center">
  <img src="assets/images/da3_radar.png" alt="Depth Anything 3" width="100%">
</p>


## ğŸ“° News
- **18-02-2026:** ğŸ¯ **æ™ºèƒ½å¯¹é½è„šæœ¬ V7 å‘å¸ƒ**ï¼é‡‡ç”¨ XY ç´§å‡‘åº¦åˆ¤å®šç®—æ³•ï¼Œå®Œç¾è§£å†³æ¡Œé¢ç‰©ä½“ vs æ¡Œåº•ä¼ªå½±çš„åŒºåˆ†é—®é¢˜ã€‚è¯¦è§ [å¯¹é½ Pipeline æŒ‡å—](docs/ALIGNMENT_PIPELINE_GUIDE.md)ã€‚
- **18-02-2026:** ğŸš€ **DA3 â†’ 3DGS åŒé‡å¯¹é½ Pipeline** ä¸Šçº¿ï¼ç»“åˆ COLMAP å’Œ Open3D çš„ä¼˜åŠ¿ï¼Œå®ç°è®­ç»ƒå‰ç²—å¯¹é½ + è®­ç»ƒåç²¾ç»†æ ¡æ­£ã€‚
- **11-12-2025:** ğŸš€ New models and [**DA3-Streaming**](da3_streaming/README.md) released! Handle ultra-long video sequence inference with less than 12GB GPU memory via sliding-window streaming inference. Special thanks to [Kai Deng](https://github.com/DengKaiCQ) for his contribution to DA3-Streaming!
- **08-12-2025:** ğŸ“Š [Benchmark evaluation pipeline](docs/BENCHMARK.md) released! Evaluate pose estimation & 3D reconstruction on 5 datasets.
- **30-11-2025:** Add [`use_ray_pose`](#use-ray-pose) and [`ref_view_strategy`](docs/funcs/ref_view_strategy.md) (reference view selection for multi-view inputs).
- **25-11-2025:** Add [Awesome DA3 Projects](#-awesome-da3-projects), a community-driven section featuring DA3-based applications.
- **14-11-2025:** Paper, project page, code and models are all released.

## âœ¨ Highlights

### ğŸ† Model Zoo
We release three series of models, each tailored for specific use cases in visual geometry.

- ğŸŒŸ **DA3 Main Series** (`DA3-Giant`, `DA3-Large`, `DA3-Base`, `DA3-Small`) These are our flagship foundation models, trained with a unified depth-ray representation. By varying the input configuration, a single model can perform a wide range of tasks:
  + ğŸŒŠ **Monocular Depth Estimation**: Predicts a depth map from a single RGB image.
  + ğŸŒŠ **Multi-View Depth Estimation**: Generates consistent depth maps from multiple images for high-quality fusion.
  + ğŸ¯ **Pose-Conditioned Depth Estimation**: Achieves superior depth consistency when camera poses are provided as input.
  + ğŸ“· **Camera Pose Estimation**:  Estimates camera extrinsics and intrinsics from one or more images.
  + ğŸŸ¡ **3D Gaussian Estimation**: Directly predicts 3D Gaussians, enabling high-fidelity novel view synthesis.

- ğŸ“ **DA3 Metric Series** (`DA3Metric-Large`) A specialized model fine-tuned for metric depth estimation in monocular settings, ideal for applications requiring real-world scale.

- ğŸ” **DA3 Monocular Series** (`DA3Mono-Large`). A dedicated model for high-quality relative monocular depth estimation. Unlike disparity-based models (e.g.,  [Depth Anything 2](https://github.com/DepthAnything/Depth-Anything-V2)), it directly predicts depth, resulting in superior geometric accuracy.

ğŸ”— Leveraging these available models, we developed a **nested series** (`DA3Nested-Giant-Large`). This series combines a any-view giant model with a metric model to reconstruct visual geometry at a real-world metric scale.

### ğŸ› ï¸ Codebase Features
Our repository is designed to be a powerful and user-friendly toolkit for both practical application and future research.
- ğŸ¨ **Interactive Web UI & Gallery**: Visualize model outputs and compare results with an easy-to-use Gradio-based web interface.
- âš¡ **Flexible Command-Line Interface (CLI)**: Powerful and scriptable CLI for batch processing and integration into custom workflows.
- ğŸ’¾ **Multiple Export Formats**: Save your results in various formats, including `glb`, `npz`, depth images, `ply`, 3DGS videos, etc, to seamlessly connect with other tools.
- ğŸ”§ **Extensible and Modular Design**: The codebase is structured to facilitate future research and the integration of new models or functionalities.


<!-- ### ğŸ¯ Visual Geometry Benchmark
We introduce a new benchmark to rigorously evaluate geometry prediction models on three key tasks: pose estimation, 3D reconstruction, and visual rendering (novel view synthesis) quality.

- ğŸ”„ **Broad Model Compatibility**: Our benchmark is designed to be versatile, supporting the evaluation of various models, including both monocular and multi-view depth estimation approaches.
- ğŸ”¬ **Robust Evaluation Pipeline**: We provide a standardized pipeline featuring RANSAC-based pose alignment, TSDF fusion for dense reconstruction, and a principled view selection strategy for novel view synthesis.
- ğŸ“Š **Standardized Metrics**: Performance is measured using established metrics: AUC for pose accuracy, F1-score and Chamfer Distance for reconstruction, and PSNR/SSIM/LPIPS for rendering quality.
- ğŸŒ **Diverse and Challenging Datasets**: The benchmark spans a wide range of scenes from datasets like HiRoom, ETH3D, DTU, 7Scenes, ScanNet++, DL3DV, Tanks and Temples, and MegaDepth. -->


## ğŸš€ Quick Start

### ğŸ“¦ Installation

```bash
pip install xformers torch\>=2 torchvision
pip install -e . # Basic
pip install --no-build-isolation git+https://github.com/nerfstudio-project/gsplat.git@0b4dddf04cb687367602c01196913cde6a743d70 # for gaussian head
pip install -e ".[app]" # Gradio, python>=3.10
pip install -e ".[all]" # ALL
```

For detailed model information, please refer to the [Model Cards](#-model-cards) section below.

### ğŸ’» Basic Usage

```python
import glob, os, torch
from depth_anything_3.api import DepthAnything3
device = torch.device("cuda")
model = DepthAnything3.from_pretrained("depth-anything/DA3NESTED-GIANT-LARGE")
model = model.to(device=device)
example_path = "assets/examples/SOH"
images = sorted(glob.glob(os.path.join(example_path, "*.png")))
prediction = model.inference(
    images,
)
# prediction.processed_images : [N, H, W, 3] uint8   array
print(prediction.processed_images.shape)
# prediction.depth            : [N, H, W]    float32 array
print(prediction.depth.shape)  
# prediction.conf             : [N, H, W]    float32 array
print(prediction.conf.shape)  
# prediction.extrinsics       : [N, 3, 4]    float32 array # opencv w2c or colmap format
print(prediction.extrinsics.shape)
# prediction.intrinsics       : [N, 3, 3]    float32 array
print(prediction.intrinsics.shape)
```

```bash

export MODEL_DIR=depth-anything/DA3NESTED-GIANT-LARGE
# This can be a Hugging Face repository or a local directory
# If you encounter network issues, consider using the following mirror: export HF_ENDPOINT=https://hf-mirror.com
# Alternatively, you can download the model directly from Hugging Face
export GALLERY_DIR=workspace/gallery
mkdir -p $GALLERY_DIR

# CLI auto mode with backend reuse
da3 backend --model-dir ${MODEL_DIR} --gallery-dir ${GALLERY_DIR} # Cache model to gpu
da3 auto assets/examples/SOH \
    --export-format glb \
    --export-dir ${GALLERY_DIR}/TEST_BACKEND/SOH \
    --use-backend

# CLI video processing with feature visualization
da3 video assets/examples/robot_unitree.mp4 \
    --fps 15 \
    --use-backend \
    --export-dir ${GALLERY_DIR}/TEST_BACKEND/robo \
    --export-format glb-feat_vis \
    --feat-vis-fps 15 \
    --process-res-method lower_bound_resize \
    --export-feat "11,21,31"

# CLI auto mode without backend reuse
da3 auto assets/examples/SOH \
    --export-format glb \
    --export-dir ${GALLERY_DIR}/TEST_CLI/SOH \
    --model-dir ${MODEL_DIR}

```

The model architecture is defined in [`DepthAnything3Net`](src/depth_anything_3/model/da3.py), and specified with a Yaml config file located at [`src/depth_anything_3/configs`](src/depth_anything_3/configs). The input and output processing are handled by [`DepthAnything3`](src/depth_anything_3/api.py). To customize the model architecture, simply create a new config file (*e.g.*, `path/to/new/config`) as:

```yaml
__object__:
  path: depth_anything_3.model.da3
  name: DepthAnything3Net
  args: as_params

net:
  __object__:
    path: depth_anything_3.model.dinov2.dinov2
    name: DinoV2
    args: as_params

  name: vitb
  out_layers: [5, 7, 9, 11]
  alt_start: 4
  qknorm_start: 4
  rope_start: 4
  cat_token: True

head:
  __object__:
    path: depth_anything_3.model.dualdpt
    name: DualDPT
    args: as_params

  dim_in: &head_dim_in 1536
  output_dim: 2
  features: &head_features 128
  out_channels: &head_out_channels [96, 192, 384, 768]
```

Then, the model can be created with the following code snippet.
```python
from depth_anything_3.cfg import create_object, load_config

Model = create_object(load_config("path/to/new/config"))
```



## ğŸ› ï¸ Community Enhancements

è¿™ä¸ª fork åœ¨åŸä»“åº“åŸºç¡€ä¸Šï¼Œæ–°å¢äº†å®Œæ•´çš„ 3D é‡å»ºç”Ÿæ€ç³»ç»Ÿï¼Œè®©ç”¨æˆ·å¯ä»¥ä»è§†é¢‘ç›´æ¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D æ¨¡å‹ã€‚

### ğŸŒŸ What's New (vs Original Repository)

**å®Œæ•´ 3D é‡å»ºç”Ÿæ€ç³»ç»Ÿ**ï¼š

1. **ğŸ¯ æ™ºèƒ½ç‚¹äº‘å¯¹é½ç³»ç»Ÿ**ï¼ˆæ–°å¢ï¼Œ2026-02-18ï¼‰
   - **V7 æ™ºèƒ½å¯¹é½è„šæœ¬**ï¼šXY ç´§å‡‘åº¦åˆ¤å®šï¼ˆæ¨èç”¨äºæ¡Œé¢ç‰©ä½“åœºæ™¯ï¼‰
     - é€‰æ‹©"åˆ†å¸ƒæ›´èšç„¦"çš„ä¸€ä¾§ä½œä¸ºæ­£é¢
     - è‡ªåŠ¨è®¡ç®—åœºæ™¯å°ºåº¦ï¼ŒåŠ¨æ€è°ƒæ•´å‚æ•°
     - æ”¯æŒæ¯«ç±³/ç±³/ä»»æ„æ¯”ä¾‹å•ä½
   - **V4 æ™ºèƒ½å¯¹é½è„šæœ¬**ï¼šDBSCAN èšç±»åˆ¤å®š
     - è‡ªé€‚åº”å°ºåº¦ + èšç±»è¿é€šæ€§åˆ†æ
     - é€‚ç”¨äºå¤æ‚ä¼ªå½±åœºæ™¯
   - **DA3 â†’ 3DGS åŒé‡å¯¹é½ Pipeline**ï¼š
     - è®­ç»ƒå‰ COLMAP å¯¹é½ï¼ˆæ›¼å“ˆé¡¿ä¸–ç•Œå‡è®¾ï¼‰
     - è®­ç»ƒå Open3D RANSAC ç²¾ç»†æ ¡æ­£
     - æ™ºèƒ½è·³è¿‡æœºåˆ¶ï¼Œé¿å…è¿‡åº¦æ—‹è½¬
   - å®Œæ•´æ–‡æ¡£ï¼š[ALIGNMENT_PIPELINE_GUIDE.md](docs/ALIGNMENT_PIPELINE_GUIDE.md)

2. **ğŸ¯ DA3 â†’ SuGaR Pipeline**ï¼ˆæ–°å¢ï¼‰
   - ä¸€é”®å°† DA3 è¾“å‡ºè½¬æ¢ä¸º SuGaR å¯ç”¨çš„ COLMAP æ ¼å¼
   - è‡ªåŠ¨å®Œæˆ 4 ä¸ªæ­¥éª¤ï¼šæ ¼å¼è½¬æ¢ â†’ äºŒè¿›åˆ¶è½¬æ¢ â†’ æ•°æ®æ•´ç† â†’ SuGaR è®­ç»ƒ
   - æ”¯æŒå¿«é€Ÿé¢„è§ˆï¼ˆ15-30åˆ†é’Ÿï¼‰ã€æ ‡å‡†è´¨é‡ï¼ˆ1å°æ—¶ï¼‰ã€é«˜è´¨é‡ï¼ˆ2å°æ—¶ï¼‰
   - è¾“å‡ºå¯ç›´æ¥åœ¨ Blender ä¸­ç¼–è¾‘æˆ–åœ¨çº¿æŸ¥çœ‹
   - å®Œæ•´æ–‡æ¡£ï¼š[DA3_TO_SUGAR_QUICKSTART.md](DA3_TO_SUGAR_QUICKSTART.md)

3. **ğŸš€ DA3 â†’ DN-Splatter Pipeline**ï¼ˆæ–°å¢ï¼‰
   - ç«¯åˆ°ç«¯ pipelineï¼šDA3 â†’ DN-Splatter â†’ 3DGS PLY
   - æ”¯æŒæ·±åº¦çº¦æŸå’Œæ³•çº¿çº¦æŸï¼Œæœ‰æ•ˆæ¶ˆé™¤ç™½å¢™æ¼‚æµ®ç‰©
   - è‡ªåŠ¨è®­ç»ƒ 30000 æ­¥å¹¶å¯¼å‡ºæ ‡å‡† PLY æ ¼å¼
   - å†…å­˜ä¼˜åŒ–ï¼šæ”¯æŒ RTX 5070ï¼Œ<12GB VRAM

4. **ğŸ“¹ Video Processing & Streaming**ï¼ˆå¢å¼ºï¼‰
   - æ‰¹é‡è§†é¢‘æ·±åº¦ä¼°è®¡å·¥å…·ï¼ˆæ”¯æŒ 720p/1080pï¼‰
   - DA3-Streaming æ”¯æŒè¶…é•¿è§†é¢‘ï¼ˆ<12GB VRAMï¼‰
   - æ»‘åŠ¨çª—å£æ¨ç† + å¾ªç¯é—­åŒ…æ£€æµ‹
   - ç”Ÿæˆé«˜è´¨é‡ç‚¹äº‘ï¼ˆPLY æ ¼å¼ï¼‰

5. **ğŸ”§ COLMAP Integration**ï¼ˆå¢å¼ºï¼‰
   - æ”¹è¿›çš„ DA3 åˆ° COLMAP æ ¼å¼è½¬æ¢å·¥å…·
   - æ”¯æŒæ–‡æœ¬æ ¼å¼å’ŒäºŒè¿›åˆ¶æ ¼å¼
   - å®Œæ•´çš„å‚æ•°éªŒè¯å’Œé”™è¯¯æç¤º
   - å…¼å®¹ SuGaRã€DN-Splatter ç­‰ä¸‹æ¸¸å·¥å…·

6. **âš¡ Performance Benchmarking**ï¼ˆæ–°å¢ï¼‰
   - å…¨é¢çš„æ€§èƒ½æµ‹è¯•å·¥å…·ï¼ˆæµ‹è¯•æ‰€æœ‰ DA3 æ¨¡å‹ï¼‰
   - RTX 5070 ä¼˜åŒ–ç»“æœ

---

### ğŸ“¦ Complete Toolset

#### ğŸ¯ 3D Gaussian Splatting Pipelines

**æ™ºèƒ½ç‚¹äº‘å¯¹é½ç³»ç»Ÿ**ï¼ˆæ–°å¢ï¼Œ2026-02-18ï¼‰
- `align_target_object_plyv7.py` - **V7 æ™ºèƒ½å¯¹é½è„šæœ¬**ï¼ˆæ¨èï¼‰
  - XY ç´§å‡‘åº¦åˆ¤å®šï¼Œé€‰æ‹©"åˆ†å¸ƒæ›´èšç„¦"çš„ä¸€ä¾§
  - é€‚ç”¨äºæ¡Œé¢ç‰©ä½“ vs æ¡Œåº•ä¼ªå½±çš„åŒºåˆ†
  - è‡ªé€‚åº”å°ºåº¦è®¡ç®—ï¼Œæ”¯æŒä»»æ„å•ä½æ¨¡å‹
- `align_target_object_ply.py` - **V4 æ™ºèƒ½å¯¹é½è„šæœ¬**
  - DBSCAN èšç±»åˆ†æï¼Œæ™ºèƒ½åˆ¤å®šæ­£åæ–¹å‘
  - é€‚ç”¨äºå¤æ‚ä¼ªå½±åœºæ™¯
- `run_da3_to_3dgs_aligned.py` - **åŒé‡å¯¹é½ Pipeline**
  - COLMAP + Open3D åŒé‡å¯¹é½
  - è®­ç»ƒå‰ç²—å¯¹é½ + è®­ç»ƒåç²¾ç»†æ ¡æ­£
- `batch_align_existing_ply.py` - æ‰¹é‡æ‰¶æ­£å·²æœ‰ PLY
- `auto_align_ply.py` - ç‹¬ç«‹æ‰¶æ­£å·¥å…·
- æ–‡æ¡£ï¼š[ALIGNMENT_PIPELINE_GUIDE.md](docs/ALIGNMENT_PIPELINE_GUIDE.md)

**DA3 Ã— SuGaR**ï¼ˆæ–°å¢ï¼Œæ¨èç”¨äºé«˜è´¨é‡é‡å»ºï¼‰
- `da3_to_sugar_pipeline.sh` - ä¸€é”®å®Œæ•´ pipeline
- `convert_da3_to_colmap.py` - DA3 è¾“å‡ºè½¬ COLMAP æ–‡æœ¬æ ¼å¼ï¼ˆå·²æ”¹è¿›ï¼‰
- `colmap_text_to_binary.py` - COLMAP æ–‡æœ¬è½¬äºŒè¿›åˆ¶æ ¼å¼
- æ–‡æ¡£ï¼š
  - [DA3_TO_SUGAR_QUICKSTART.md](DA3_TO_SUGAR_QUICKSTART.md) - å¿«é€Ÿå¼€å§‹
  - [DA3_TO_SUGAR_PIPELINE.md](DA3_TO_SUGAR_PIPELINE.md) - å®Œæ•´æŒ‡å—
  - [DA3_TO_SUGAR_IMPLEMENTATION.md](DA3_TO_SUGAR_IMPLEMENTATION.md) - å®ç°ç»†èŠ‚

**DA3 Ã— DN-Splatter**ï¼ˆæ–°å¢ï¼Œæ¨èç”¨äºå¿«é€Ÿé‡å»ºï¼‰
- `run_da3_to_dn_splatter_pipeline.py` - ç«¯åˆ°ç«¯ pipeline
- `run_da3_to_dn_splatter.py` - ç‹¬ç«‹è½¬æ¢å·¥å…·
- `run_direct_dn_splatter.py` - ç›´æ¥ DN-Splatter è®­ç»ƒ
- `batch_export_ply.py` - æ‰¹é‡å¯¼å‡º PLY
- æ–‡æ¡£ï¼š[DN_SPLATTER_PIPELINE_GUIDE.md](docs/DN_SPLATTER_PIPELINE_GUIDE.md)

**Classic 3DGS**ï¼ˆæ–°å¢ï¼Œç›´æ¥ç”Ÿæˆï¼‰
- `generate_3dgs.py` - ç›´æ¥ä» DA3 è¾“å‡ºç”Ÿæˆ 3DGS
- `run_da3_3dgs.sh` - è‡ªåŠ¨åŒ– pipeline
- `run_gradio_direct.sh` - Gradio UI
- æ–‡æ¡£ï¼š[DA3_3DGS_GUIDE.md](DA3_3DGS_GUIDE.md)

#### ğŸ“¹ Video Processing & Streaming

**Video Depth Estimation**
- `process_video.py` - æ‰¹é‡è§†é¢‘æ·±åº¦ä¼°è®¡
  - æ”¯æŒå¯é…ç½® FPS æå–
  - æ”¯æŒ 720pã€1080p åˆ†è¾¨ç‡
  - å¯¼å‡ºæ·±åº¦å›¾ã€ç½®ä¿¡åº¦å›¾ã€å¤„ç†åçš„å¸§

**Long Video Streaming**
- `run_sugar_streaming.sh` - DA3-Streaming å¤„ç†è¶…é•¿è§†é¢‘
  - åˆ†å—å¤„ç† + é‡å ï¼ˆå†…å­˜é«˜æ•ˆï¼‰
  - å¾ªç¯é—­åŒ…æ£€æµ‹
  - ç”Ÿæˆ PLY ç‚¹äº‘
- æ–‡æ¡£ï¼š[DA3_STREAMING_GUIDE.md](DA3_STREAMING_GUIDE.md)

#### ğŸ”§ Format Conversion & Integration

**COLMAP Integration**ï¼ˆå¢å¼ºï¼‰
- `convert_da3_to_colmap.py` - DA3 è¾“å‡ºè½¬ COLMAP æ ¼å¼ï¼ˆå·²æ”¹è¿›ï¼‰
  - âœ… å®Œæ•´çš„å‚æ•°éªŒè¯å’Œé”™è¯¯æç¤º
  - âœ… æ”¯æŒä¸­æ–‡æ³¨é‡Šå’Œè¾“å‡º
  - âœ… è‡ªåŠ¨ç¬¦å·é“¾æ¥æˆ–å¤åˆ¶å›¾åƒ
  - âœ… æ”¯æŒæ–‡æœ¬æ ¼å¼å’ŒäºŒè¿›åˆ¶æ ¼å¼è½¬æ¢
- `colmap_text_to_binary.py` - COLMAP æ–‡æœ¬è½¬äºŒè¿›åˆ¶

**å…¶ä»–é›†æˆ**
- `run_da3_glomap_pipeline.py` - GLOMAP é›†æˆ pipeline
- `run_da3_to_3dgs_direct.py` - ç›´æ¥ DA3 åˆ° 3DGS è½¬æ¢

#### ğŸ“Š Performance & Testing

**Benchmarking**
- `benchmark.py` - å…¨é¢çš„æ€§èƒ½æµ‹è¯•å·¥å…·
  - æµ‹è¯•æ‰€æœ‰ DA3 æ¨¡å‹å¤§å°ï¼ˆSMALL, BASE, LARGE, GIANTï¼‰
  - æµ‹é‡æ¨ç†æ—¶é—´ã€FPSã€VRAM ä½¿ç”¨
- æ–‡æ¡£ï¼š[PERFORMANCE_REPORT.md](PERFORMANCE_REPORT.md)

**Testing & Utilities**
- `test_inference.py` - å¿«é€Ÿæ¨ç†æµ‹è¯•
- `inspect_npz.py` - NPZ æ–‡ä»¶æ£€æŸ¥å·¥å…·
- `cleaned_help.txt`, `ns_help.txt` - é¢å¤–æ–‡æ¡£

#### ğŸ“– Installation & Documentation

**Setup Guides**
- [REPRODUCTION.md](REPRODUCTION.md) - å®Œæ•´å®‰è£…æŒ‡å—
  - WSL2 + CUDA 12.8 ç¯å¢ƒè®¾ç½®
  - Bug ä¿®å¤ï¼ˆmoviepy å¯¼å…¥ã€HF é•œåƒï¼‰
  - æ¨¡å‹ä¸‹è½½è¯´æ˜
- [REPRODUCTION_SUMMARY.md](REPRODUCTION_SUMMARY.md) - å¿«é€Ÿå‚è€ƒ

**Model Weights**
- `weights/model.safetensors` - DA3 æ¨¡å‹æ£€æŸ¥ç‚¹
- `weights/config.json` - æ¨¡å‹é…ç½®
- `weights/dino_salad.ckpt` - SALAD æƒé‡

---

### ğŸ“ è¾“å‡ºç›®å½•ç»“æ„

#### DA3 ä¸»è¾“å‡ºç›®å½•

```
output/
â”œâ”€â”€ sugar_streaming/              # SuGaR streaming è¾“å‡ºï¼ˆé»˜è®¤ï¼‰
â”‚   â”œâ”€â”€ camera_poses.ply         # ç›¸æœºä½å§¿ç‚¹äº‘
â”‚   â”œâ”€â”€ camera_poses.txt         # ç›¸æœºä½å§¿æ–‡æœ¬
â”‚   â”œâ”€â”€ intrinsic.txt            # ç›¸æœºå†…å‚
â”‚   â”œâ”€â”€ loop_closures.txt        # é—­ç¯æ£€æµ‹ç»“æœ
â”‚   â”œâ”€â”€ colmap_data/            # COLMAP æ•°æ®ï¼ˆäºŒè¿›åˆ¶ï¼‰
â”‚   â”œâ”€â”€ colmap_text/            # COLMAP æ•°æ®ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
â”‚   â”œâ”€â”€ glomap_ws/              # GLOMAP å·¥ä½œç©ºé—´
â”‚   â”œâ”€â”€ pcd/                   # ç‚¹äº‘æ–‡ä»¶
â”‚   â”œâ”€â”€ extracted/              # æå–çš„å›¾åƒ
â”‚   â”œâ”€â”€ results_output/         # ç»“æœè¾“å‡º
â”‚   â”œâ”€â”€ da3_3dgs_pipeline/     # 3DGS è®­ç»ƒè¾“å‡º
â”‚   â”œâ”€â”€ da3_3dgs_colmap_aligned_pipeline/   # COLMAP å¯¹é½åçš„3DGS
â”‚   â””â”€â”€ da3_3dgs_aligned_pipeline/         # èåˆå¯¹é½åçš„3DGS
â”œâ”€â”€ video_depth/                # è§†é¢‘æ·±åº¦ä¼°è®¡è¾“å‡º
â”œâ”€â”€ video_test/                # æµ‹è¯•è§†é¢‘è¾“å‡º
â”œâ”€â”€ sugar_video/               # SuGaR è§†é¢‘å¤„ç†è¾“å‡º
â”œâ”€â”€ da3_3dgs/                # DA3 3DGS è¾“å‡º
â”œâ”€â”€ quick_mesh/               # å¿«é€Ÿmeshè¾“å‡º
â”œâ”€â”€ da3_dn_splatter_dataset/    # DA3â†’DN-Splatter æ•°æ®é›†
â””â”€â”€ da3_dn_splatter_output/    # DA3â†’DN-Splatter è¾“å‡º
```

#### è·¨é¡¹ç›®è¾“å‡º

- **SuGaR**: `~/projects/SuGaR/output/3dgs/` - DA3â†’3DGS PLY
- **DN-Splatter**: `~/projects/dn-splatter/output/` - DN-Splatter è®­ç»ƒè¾“å‡º

### ğŸ”— æ•°æ®æµå‘ä¸é›†æˆ

#### DA3 â†’ 3DGS Pipeline

```
è§†é¢‘ â†’ DA3 (æ·±åº¦ä¼°è®¡+ä½å§¿ä¼°è®¡) â†’ colmap_text/ â†’ 3DGS è®­ç»ƒ â†’ PLY æ–‡ä»¶
```

#### è·¨é¡¹ç›®é›†æˆ

```
DA3 output/
â”œâ”€â”€ colmap_text/              # æ–‡æœ¬æ ¼å¼COLMAPæ•°æ®
â”‚   â”œâ”€â”€â”€â–º SuGaR/output/3dgs/     (DA3â†’3DGS PLY)
â”‚   â””â”€â”€â”€â–º dn-splatter/            # æ·±åº¦å…ˆéªŒ
â””â”€â”€ camera_poses.txt
```

#### è¾“å‡ºæ•°æ®å±‚çº§

```
åŸºç¡€å±‚ (DA3)
  â”œâ”€â”€ colmap_text/              # æ–‡æœ¬æ ¼å¼COLMAPæ•°æ®
  â”œâ”€â”€ colmap_data/              # äºŒè¿›åˆ¶æ ¼å¼COLMAPæ•°æ®
  â”œâ”€â”€ camera_poses.txt         # ç›¸æœºä½å§¿æ–‡æœ¬
  â”œâ”€â”€ pcd/                     # ç‚¹äº‘æ–‡ä»¶
  â””â”€â”€ depthå›¾åƒ               # æ·±åº¦ä¼°è®¡ç»“æœ

ä¸­çº§å±‚ (3DGSè®­ç»ƒ)
  â”œâ”€â”€ da3_3dgs_pipeline/       # çº¯3DGSè®­ç»ƒè¾“å‡º
  â”œâ”€â”€ da3_2dgs_pipeline/       # 2DGSè®­ç»ƒè¾“å‡º
  â””â”€â”€ da3_dn_splatter_dataset/   # DN-Splatteræ•°æ®é›†

é«˜çº§å±‚ (é«˜è´¨é‡è¾“å‡º)
  â”œâ”€â”€ SuGaR/output/             # SuGaRè®­ç»ƒè¾“å‡ºï¼ˆè·¨é¡¹ç›®ï¼‰
  â”‚   â”œâ”€â”€ vanilla_gs/           # Vanilla 3DGS
  â”‚   â”œâ”€â”€ coarse/               # Coarse SuGaR
  â”‚   â”œâ”€â”€ refined/              # Refined SuGaR
  â”‚   â””â”€â”€ refined_mesh/         # Meshè¾“å‡º
  â””â”€â”€ dn-splatter/output/       # DN-Splatterè¾“å‡ºï¼ˆè·¨é¡¹ç›®ï¼‰

åå¤„ç†å±‚ (å¯¹é½å’Œä¼˜åŒ–)
  â”œâ”€â”€ da3_3dgs_aligned_pipeline/        # COLMAPå¯¹é½
  â”œâ”€â”€ da3_3dgs_colmap_aligned_pipeline/ # Open3Då¯¹é½
  â””â”€â”€ da3_3dgs_aligned_pipeline/       # åŒé‡å¯¹é½
```

### ğŸ¯ å¿«é€Ÿé€‰æ‹©æŒ‡å—

#### æ ¹æ®éœ€æ±‚é€‰æ‹©è„šæœ¬

| éœ€æ±‚ | æ¨èè„šæœ¬ | é¢„è®¡æ—¶é—´ | è¯´æ˜ |
|------|---------|---------|------|
| æœ€å¿«é€Ÿåº¦è·å–3DGS | `da3_to_3dgs.sh` | 15-30åˆ†é’Ÿ | çº¯3DGSè®­ç»ƒ |
| é«˜è´¨é‡å‡ ä½• | `da3_to_2dgs.sh` | 15-30åˆ†é’Ÿ | 2DGSï¼Œå‡ ä½•è´¨é‡æ›´å¥½ |
| æœ€é«˜è´¨é‡+Mesh | `da3_to_sugar_pipeline.sh` | 2-3å°æ—¶ | SuGaRå®Œæ•´æµç¨‹ |
| æ¨èæ–¹æ¡ˆï¼ˆåŒé‡å¯¹é½ï¼‰ | `run_da3_to_3dgs_aligned.py` | 15-30åˆ†é’Ÿ | COLMAP+Open3D |
| æ‰¹é‡æ‰¶æ­£å·²æœ‰PLY | `batch_align_existing_ply.py` | - | æ‰¹é‡å¯¹é½ |

#### å¯¹é½è„šæœ¬é€‰æ‹©

| åœºæ™¯ | æ¨èè„šæœ¬ | åŸå›  |
|------|---------|------|
| æ¡Œé¢ç‰©ä½“åœºæ™¯ | `align_target_object_plyv7.py` | XYç´§å‡‘åº¦åˆ¤å®šï¼ŒåŒºåˆ†ç‰©ä½“vsæ¡Œåº• |
| å¤æ‚ä¼ªå½±åœºæ™¯ | `align_target_object_ply.py` | DBSCANèšç±»ï¼Œå¤„ç†ä¼ªå½± |
| è®­ç»ƒå‰ç²—å¯¹é½ | `da3_to_3dgs_aligned_colmap.sh` | COLMAP model_aligner |
| è®­ç»ƒåç²¾ç»†æ ¡æ­£ | `da3_to_3dgs_aligned_open3d.sh` | Open3D RANSAC |
| åŒé‡å¯¹é½ï¼ˆæ¨èï¼‰ | `run_da3_to_3dgs_aligned.py` | COLMAPï¼ˆè®­ç»ƒå‰ï¼‰+ Open3Dï¼ˆè®­ç»ƒåï¼‰ |

---

### ğŸš€ Getting Started

#### å¿«é€Ÿå¼€å§‹ - æ¨èæµç¨‹

**1. åŸºç¡€æ¨ç†æµ‹è¯•**
```bash
python test_inference.py
```

**2. SuGaR Pipelineï¼ˆæ¨èç”¨äºé«˜è´¨é‡ 3D é‡å»ºï¼‰**
```bash
cd /home/ltx/projects/Depth-Anything-3

# å¿«é€Ÿé¢„è§ˆï¼ˆçº¦30åˆ†é’Ÿï¼‰
./da3_to_sugar_pipeline.sh output/sugar_streaming my_scene dn_consistency short false true

# æ ‡å‡†è´¨é‡ï¼ˆçº¦1å°æ—¶ï¼‰
./da3_to_sugar_pipeline.sh output/sugar_streaming my_scene dn_consistency short true false
```

**3. DN-Splatter Pipelineï¼ˆæ¨èç”¨äºå¿«é€Ÿé‡å»ºï¼‰**
```bash
python run_da3_to_dn_splatter_pipeline.py
```

**4. è§†é¢‘æ·±åº¦ä¼°è®¡**
```bash
# ç¼–è¾‘ process_video.py ä¸­çš„ VIDEO_PATH
python process_video.py
```

**5. é•¿è§†é¢‘æµå¤„ç†**
```bash
# ç¼–è¾‘ run_sugar_streaming.sh ä¸­çš„ VIDEO_PATH
bash run_sugar_streaming.sh
```

**6. æ€§èƒ½æµ‹è¯•**
```bash
python benchmark.py
```

---

### âœ¨ Features & Benefits

âœ… **å®Œæ•´ç”Ÿæ€** - ä»è§†é¢‘åˆ° 3D æ¨¡å‹çš„å®Œæ•´ pipeline
âœ… **å†…å­˜é«˜æ•ˆ** - DA3-Streaming æ”¯æŒè¶…é•¿è§†é¢‘ï¼ˆ<12GB VRAMï¼‰
âœ… **å¾ªç¯é—­åŒ…** - SIM3 ä¼˜åŒ–é˜²æ­¢æ¼‚ç§»
âœ… **æ‰¹é‡å¤„ç†** - è‡ªåŠ¨åŒ–è§†é¢‘å¸§æå–å’Œå¤„ç†
âœ… **COLMAP å°±ç»ª** - ç›´æ¥æ ¼å¼è½¬æ¢ç”¨äºä¸‹æ¸¸å·¥å…·
âœ… **RTX 5070 ä¼˜åŒ–** - é’ˆå¯¹æœ€æ–° GPU æ¶æ„ä¼˜åŒ–
âœ… **ä¸­æ–‡æ”¯æŒ** - å·¥å…·å’Œæ–‡æ¡£åŒ…å«ä¸­æ–‡æ³¨é‡Š
âœ… **è¯¦ç»†æ–‡æ¡£** - æ¯ä¸ªåŠŸèƒ½éƒ½æœ‰å®Œæ•´çš„ä½¿ç”¨æŒ‡å—

---

**Fork by**: [@tianxingleo](https://github.com/tianxingleo)
**Last Updated**: 2026-02-18
**Tested on**: WSL2 Ubuntu, CUDA 12.8, RTX 5070
**License**: Inherits from original repository

## ğŸ“š Useful Documentation

### Pipeline & Usage
- ğŸ¯ [ç‚¹äº‘è‡ªåŠ¨æ‰¶æ­£ Pipeline æŒ‡å—](docs/ALIGNMENT_PIPELINE_GUIDE.md) - **æ™ºèƒ½å¯¹é½ç³»ç»Ÿå®Œæ•´æ–‡æ¡£**
- ğŸ¯ [DA3 â†’ SuGaR Pipeline Guide](DA3_TO_SUGAR_PIPELINE.md) - å®Œæ•´pipelineæ–‡æ¡£
- âš¡ [DA3 â†’ SuGaR Quick Start](DA3_TO_SUGAR_QUICKSTART.md) - å¿«é€Ÿå¼€å§‹æŒ‡å—
- ğŸ¯ [SuGaR Modes Technical Details](SUGAR_MODES_TECHNICAL_DETAILS.md) - æ¨¡å¼æŠ€æœ¯å¯¹æ¯”
- ğŸ§  [SDF Regularization Guide](SDF_REGULARIZATION_GUIDE.md) - **SDFçº¦æŸè¯¦ç»†ä½¿ç”¨æŒ‡å—**
- ğŸ“‹ [SuGaR Official Default Iterations](SUGAR_OFFICIAL_DEFAULT_ITERATIONS.md) - å®˜æ–¹é»˜è®¤è¿­ä»£æ¬¡æ•°

### Core Documentation
- ğŸ–¥ï¸ [Command Line Interface](docs/CLI.md)
- ğŸ“‘ [Python API](docs/API.md)
- ğŸ“Š [Benchmark Evaluation](docs/BENCHMARK.md)

## ğŸ—‚ï¸ Model Cards

Generally, you should observe that DA3-LARGE achieves comparable results to VGGT.

The Nested series uses an Any-view model to estimate pose and depth, and a monocular metric depth estimator for scaling. 

âš ï¸ Models with the `-1.1` suffix are retrained after fixing a training bug; prefer these refreshed checkpoints. The original `DA3NESTED-GIANT-LARGE`, `DA3-GIANT`, and `DA3-LARGE` remain available but are deprecated. You could expect much better performance for street scenes with the `-1.1` models.

| ğŸ—ƒï¸ Model Name                  | ğŸ“ Params | ğŸ“Š Rel. Depth | ğŸ“· Pose Est. | ğŸ§­ Pose Cond. | ğŸ¨ GS | ğŸ“ Met. Depth | â˜ï¸ Sky Seg | ğŸ“„ License     |
|-------------------------------|-----------|---------------|--------------|---------------|-------|---------------|-----------|----------------|
| **Nested** | | | | | | | | |
| [DA3NESTED-GIANT-LARGE-1.1](https://huggingface.co/depth-anything/DA3NESTED-GIANT-LARGE-1.1)  | 1.40B     | âœ…             | âœ…            | âœ…             | âœ…     | âœ…             | âœ…         | CC BY-NC 4.0   |
| [DA3NESTED-GIANT-LARGE](https://huggingface.co/depth-anything/DA3NESTED-GIANT-LARGE)  | 1.40B     | âœ…             | âœ…            | âœ…             | âœ…     | âœ…             | âœ…         | CC BY-NC 4.0   |
| **Any-view Model** | | | | | | | | |
| [DA3-GIANT-1.1](https://huggingface.co/depth-anything/DA3-GIANT-1.1)                     | 1.15B     | âœ…             | âœ…            | âœ…             | âœ…     |               |           | CC BY-NC 4.0   |
| [DA3-GIANT](https://huggingface.co/depth-anything/DA3-GIANT)                     | 1.15B     | âœ…             | âœ…            | âœ…             | âœ…     |               |           | CC BY-NC 4.0   |
| [DA3-LARGE-1.1](https://huggingface.co/depth-anything/DA3-LARGE-1.1)                     | 0.35B     | âœ…             | âœ…            | âœ…             |       |               |           | CC BY-NC 4.0     |
| [DA3-LARGE](https://huggingface.co/depth-anything/DA3-LARGE)                     | 0.35B     | âœ…             | âœ…            | âœ…             |       |               |           | CC BY-NC 4.0     |
| [DA3-BASE](https://huggingface.co/depth-anything/DA3-BASE)                     | 0.12B     | âœ…             | âœ…            | âœ…             |       |               |           | Apache 2.0     |
| [DA3-SMALL](https://huggingface.co/depth-anything/DA3-SMALL)                     | 0.08B     | âœ…             | âœ…            | âœ…             |       |               |           | Apache 2.0     |
|                               |           |               |              |               |               |       |           |                |
| **Monocular Metric Depth** | | | | | | | | |
| [DA3METRIC-LARGE](https://huggingface.co/depth-anything/DA3METRIC-LARGE)              | 0.35B     | âœ…             |              |               |       | âœ…             | âœ…         | Apache 2.0     |
|                               |           |               |              |               |               |       |           |                |
| **Monocular Depth** | | | | | | | | |
| [DA3MONO-LARGE](https://huggingface.co/depth-anything/DA3MONO-LARGE)                | 0.35B     | âœ…             |              |               |               |       | âœ…         | Apache 2.0     |


## â“ FAQ

- **Monocular Metric Depth**: To obtain metric depth in meters from `DA3METRIC-LARGE`, use `metric_depth = focal * net_output / 300.`, where `focal` is the focal length in pixels (typically the average of fx and fy from the camera intrinsic matrix K). Note that the output from `DA3NESTED-GIANT-LARGE` is already in meters.

- <a id="use-ray-pose"></a>**Ray Head (`use_ray_pose`)**:  Our API and CLI support `use_ray_pose` arg, which means that the model will derive camera pose from ray head, which is generally slightly slower, but more accurate. Note that the default is `False` for faster inference speed. 
  <details>
  <summary>AUC3 Results for DA3NESTED-GIANT-LARGE</summary>
  
  | Model | HiRoom | ETH3D | DTU | 7Scenes | ScanNet++ | 
  |-------|------|-------|-----|---------|-----------|
  | `ray_head` | 84.4 | 52.6 | 93.9 | 29.5 | 89.4 |
  | `cam_head` | 80.3 | 48.4 | 94.1 | 28.5 | 85.0 |

  </details>




- **Older GPUs without XFormers support**: See [Issue #11](https://github.com/ByteDance-Seed/Depth-Anything-3/issues/11). Thanks to [@S-Mahoney](https://github.com/S-Mahoney) for the solution!


## ğŸ¢ Awesome DA3 Projects

A community-curated list of Depth Anything 3 integrations across 3D tools, creative pipelines, robotics, and web/VR viewers, including but not limited to these. You are welcome to submit your DA3-based project via PR, and we will review and feature it if applicable.

- [DA3-blender](https://github.com/xy-gao/DA3-blender): Blender addon for DA3-based 3D reconstruction from a set of images. 

- [ComfyUI-DepthAnythingV3](https://github.com/PozzettiAndrea/ComfyUI-DepthAnythingV3): ComfyUI nodes for Depth Anything 3, supporting single/multi-view and video-consistent depth with optional pointâ€‘cloud export.

- [DA3-ROS2-Wrapper](https://github.com/GerdsenAI/GerdsenAI-Depth-Anything-3-ROS2-Wrapper): Real-time DA3 depth in ROS2 with multi-camera support. 

- [DA3-ROS2-CPP-TensorRT](https://github.com/ika-rwth-aachen/ros2-depth-anything-v3-trt): DA3 ROS2 C++ TensorRT Inference Node: a ROS2 node for DA3 depth estimation using TensorRT for real-time inference.

- [VideoDepthViewer3D](https://github.com/amariichi/VideoDepthViewer3D): Streaming videos with DA3 metric depth to a Three.js/WebXR 3D viewer for VR/stereo playback.


## ğŸ§‘â€ğŸ’» Official Codebase Core Contributors and Maintainers

<table>
  <tr>
    <td align="center">
      <a href="https://bingykang.github.io/">
        <img src="https://images.weserv.nl/?url=https://bingykang.github.io/images/bykang_homepage.jpeg?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub><b>Bingyi Kang</b></sub>
    </td>
    <td align="center">
      <a href="https://haotongl.github.io/">
        <img src="https://images.weserv.nl/?url=https://haotongl.github.io/assets/img/prof_pic.jpg?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Haotong Lin</sub>
    </td>
    <td align="center">
      <a href="https://github.com/SiliChen321">
        <img src="https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/195901058?v=4&h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Sili Chen</sub>
    </td>
    <td align="center">
      <a href="https://liewjunhao.github.io/">
        <img src="https://images.weserv.nl/?url=https://liewjunhao.github.io/images/liewjunhao.png?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
       </a>
        <br />
        <sub>Jun Hao Liew</sub>
    </td>
    <td align="center">
      <a href="https://donydchen.github.io/">
        <img src="https://images.weserv.nl/?url=https://donydchen.github.io/assets/img/profile.jpg?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Donny Y. Chen</sub>
    </td>
    <td align="center">
      <a href="https://github.com/DengKaiCQ">
        <img src="https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/59907452?v=4&h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Kai Deng</sub>
    </td>
  </tr>
</table>

## ğŸ“ Citations
If you find Depth Anything 3 useful in your research or projects, please cite our work:

```
@article{depthanything3,
  title={Depth Anything 3: Recovering the visual space from any views},
  author={Haotong Lin and Sili Chen and Jun Hao Liew and Donny Y. Chen and Zhenyu Li and Guang Shi and Jiashi Feng and Bingyi Kang},
  journal={arXiv preprint arXiv:2511.10647},
  year={2025}
}
```
