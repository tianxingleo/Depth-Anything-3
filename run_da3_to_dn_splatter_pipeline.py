"""
DA3 â†’ DN-Splatter ç»Ÿä¸€ Pipeline
================================
å®Œæ•´æµç¨‹:
  1) å°† Depth-Anything-3 çš„è¾“å‡º (å›¾ç‰‡ + Depth NPZ + Poses) è½¬æ¢ä¸º DN-Splatter æ‰€éœ€æ ¼å¼
     - transforms.json (Nerfstudioæ ¼å¼çš„ç›¸æœºå‚æ•°)
     - images/ (åŸå›¾)
     - depths/ (16-bit PNG æ¯«ç±³æ·±åº¦å›¾)
     - normals_from_pretrain/ (ä»æ·±åº¦å›¾ç”Ÿæˆçš„æ³•çº¿è´´å›¾)
  2) ä½¿ç”¨ ns-train dn-splatter è®­ç»ƒ
  3) å¯¼å‡º PLY

ç”¨æ³•:
    python run_da3_to_dn_splatter_pipeline.py [--source_dir PATH] [--output_name NAME]

ä¾èµ–è§£æ:
    - æ•°æ®è½¬æ¢æ­¥éª¤: numpy, Pillow, tqdm (å½“å‰ python)
    - è®­ç»ƒæ­¥éª¤: nerfstudio + dn_splatter (gs_linux_backup ç¯å¢ƒ)
"""

import argparse
import json
import os
import shutil
import subprocess
import sys
import time
from pathlib import Path

import numpy as np
from PIL import Image
from tqdm import tqdm

# ================= ğŸ”§ é»˜è®¤è·¯å¾„é…ç½® =================
DEFAULT_SOURCE_DIR = Path("/home/ltx/projects/Depth-Anything-3/output/sugar_streaming")
DEFAULT_OUTPUT_NAME = "da3_dn_splatter"

# ç¯å¢ƒè·¯å¾„ (DN-Splatter å®‰è£…åœ¨ gs_linux_backup ç¯å¢ƒä¸­)
CONDA_PREFIX = "/home/ltx/my_envs/gs_linux_backup"
NS_ENV_BIN = f"{CONDA_PREFIX}/bin"
NS_PYTHON_EXE = f"{NS_ENV_BIN}/python"
NS_TRAIN = f"{NS_ENV_BIN}/ns-train"
NS_EXPORT = f"{NS_ENV_BIN}/ns-export"

PROJECT_ROOT = Path("/home/ltx/projects/Depth-Anything-3")


# ================= Step 1: æ•°æ®æ ¼å¼è½¬æ¢ =================

def depth_to_normal(depth, K):
    """
    ä»æ·±åº¦å›¾è®¡ç®—è¡¨é¢æ³•çº¿ (çº¯ numpy å®ç°)
    depth: float32, (H, W) ä»¥ç±³ä¸ºå•ä½
    K: (fx, fy, cx, cy)
    Returns: normal_img (H, W, 3) uint8 [0, 255]
    """
    fx, fy, cx, cy = K
    h, w = depth.shape

    # Central difference gradients
    zy, zx = np.gradient(depth)

    # Scale gradients by focal length/depth to account for perspective
    scale_x = fx / (depth + 1e-6)
    scale_y = fy / (depth + 1e-6)

    nx = -zx * scale_x
    ny = -zy * scale_y
    nz = np.ones_like(depth)

    # Normalize
    n = np.sqrt(nx**2 + ny**2 + nz**2)
    n[n == 0] = 1.0

    normal = np.dstack((nx / n, ny / n, nz / n))

    # Map [-1, 1] -> [0, 255]
    normal_img = ((normal + 1) / 2 * 255).astype(np.uint8)
    return normal_img


def load_intrinsics(path):
    """è¯»å–å†…å‚æ–‡ä»¶ (å•è¡Œ: fx fy cx cy)"""
    print(f"  ğŸ“ åŠ è½½å†…å‚: {path}")
    with open(path, "r") as f:
        lines = f.readlines()
        parts = (
            lines[0].strip().split()
            if "," not in lines[0]
            else lines[0].strip().split(",")
        )
        return float(parts[0]), float(parts[1]), float(parts[2]), float(parts[3])


def load_poses(path):
    """è¯»å–ç›¸æœºä½å§¿æ–‡ä»¶ (æ¯å¸§16ä¸ªæ•°å­— = 4x4çŸ©é˜µ, æŒ‰è¡Œæ’åˆ—)"""
    print(f"  ğŸ“· åŠ è½½ä½å§¿: {path}")
    poses = []
    with open(path, "r") as f:
        for line in f:
            nums = list(map(float, line.strip().split()))
            poses.append(np.array(nums).reshape(4, 4))
    return poses


def convert_da3_to_dn_splatter(source_dir: Path, dataset_dir: Path):
    """
    å°† DA3 è¾“å‡ºè½¬æ¢ä¸º DN-Splatter å¯æ¥å—çš„ Nerfstudio JSON æ•°æ®æ ¼å¼
    """
    print("=" * 60)
    print("ğŸ“¦ [Step 1] DA3 â†’ DN-Splatter æ•°æ®æ ¼å¼è½¬æ¢")
    print("=" * 60)

    extracted_dir = source_dir / "extracted"
    results_dir = source_dir / "results_output"

    if not source_dir.exists():
        raise FileNotFoundError(f"æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
    if not extracted_dir.exists():
        raise FileNotFoundError(f"æå–å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {extracted_dir}")
    if not results_dir.exists():
        raise FileNotFoundError(f"æ·±åº¦ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_images = dataset_dir / "images"
    out_depths = dataset_dir / "depths"
    out_normals = dataset_dir / "normals_from_pretrain"

    for d in [out_images, out_depths, out_normals]:
        d.mkdir(parents=True, exist_ok=True)

    # 1. åŠ è½½å…ƒæ•°æ®
    fx, fy, cx, cy = load_intrinsics(source_dir / "intrinsic.txt")
    poses = load_poses(source_dir / "camera_poses.txt")

    # 2. åŒ¹é…æ–‡ä»¶
    img_files = sorted(list(extracted_dir.glob("*.png")))
    npz_files = sorted(
        list(results_dir.glob("*.npz")), key=lambda x: int(x.stem.split("_")[1])
    )

    if not img_files:
        raise ValueError(f"åœ¨ {extracted_dir} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡")
    if not npz_files:
        raise ValueError(f"åœ¨ {results_dir} ä¸­æœªæ‰¾åˆ° NPZ æ·±åº¦æ–‡ä»¶")

    num_frames = min(len(img_files), len(npz_files), len(poses))
    if len(img_files) != len(npz_files):
        print(
            f"  âš ï¸  å›¾ç‰‡æ•°é‡ ({len(img_files)}) ä¸ NPZ æ•°é‡ ({len(npz_files)}) ä¸åŒ¹é…ï¼Œ"
            f"ä½¿ç”¨å‰ {num_frames} å¸§"
        )

    # 3. æ£€æµ‹å›¾ç‰‡å®é™…åˆ†è¾¨ç‡ (PIL returns width, height)
    first_img = Image.open(img_files[0])
    img_w, img_h = first_img.size
    print(f"  ğŸ“ å›¾ç‰‡åˆ†è¾¨ç‡: {img_w} x {img_h}")

    # 4. æ£€æµ‹æ·±åº¦å›¾åˆ†è¾¨ç‡
    first_depth = np.load(npz_files[0])["depth"]
    depth_h, depth_w = first_depth.shape
    print(f"  ğŸ“ æ·±åº¦å›¾åˆ†è¾¨ç‡: {depth_w} x {depth_h}")

    # 5. è®¡ç®—ç¼©æ”¾å› å­ & è°ƒæ•´å†…å‚
    # å†…å‚æ˜¯åŸºäºæ·±åº¦å›¾åˆ†è¾¨ç‡çš„ï¼Œéœ€è¦ç¼©æ”¾åˆ°å›¾ç‰‡åˆ†è¾¨ç‡
    if (depth_w, depth_h) != (img_w, img_h):
        scale_x = img_w / depth_w
        scale_y = img_h / depth_h
        print(f"  ğŸ”„ åˆ†è¾¨ç‡ä¸åŒ¹é…! ç¼©æ”¾æ¯”: x={scale_x:.4f}, y={scale_y:.4f}")
        fx_scaled = fx * scale_x
        fy_scaled = fy * scale_y
        cx_scaled = cx * scale_x
        cy_scaled = cy * scale_y
        print(f"  ğŸ“ è°ƒæ•´åå†…å‚: fx={fx_scaled:.2f}, fy={fy_scaled:.2f}, cx={cx_scaled:.2f}, cy={cy_scaled:.2f}")
        need_resize = True
    else:
        fx_scaled, fy_scaled, cx_scaled, cy_scaled = fx, fy, cx, cy
        need_resize = False

    print(f"  ğŸï¸  å¤„ç† {num_frames} å¸§...")

    # OpenCV â†’ OpenGL åæ ‡ç³»è½¬æ¢çŸ©é˜µ
    flip_mat = np.array([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])

    frames_data = []

    for i in tqdm(range(num_frames), desc="  è½¬æ¢ä¸­"):
        src_img_path = img_files[i]
        src_npz_path = npz_files[i]
        pose = poses[i]

        frame_name = f"frame_{i:05d}"
        dst_name = f"{frame_name}.png"

        # 1) å¤åˆ¶å›¾ç‰‡
        shutil.copy2(src_img_path, out_images / dst_name)

        # 2) å¤„ç†æ·±åº¦å›¾ (NPZ â†’ 16-bit uint16 PNG, æ¯«ç±³)
        data = np.load(src_npz_path)
        depth_map = data["depth"]  # float32, meters

        # Resize depth to image resolution if needed
        if need_resize:
            depth_pil = Image.fromarray(depth_map)
            depth_pil = depth_pil.resize((img_w, img_h), Image.BILINEAR)
            depth_map = np.array(depth_pil, dtype=np.float32)

        depth_mm = (depth_map * 1000).astype(np.uint16)
        Image.fromarray(depth_mm).save(out_depths / dst_name)

        # 3) ä»æ·±åº¦å›¾ç”Ÿæˆæ³•çº¿å›¾ (ä½¿ç”¨ç¼©æ”¾åçš„å†…å‚)
        normal_map = depth_to_normal(depth_map, (fx_scaled, fy_scaled, cx_scaled, cy_scaled))
        Image.fromarray(normal_map).save(out_normals / dst_name)

        # 4) å¤„ç†ä½å§¿
        if np.isinf(pose).any() or np.isnan(pose).any():
            print(f"  âš ï¸  ç¬¬ {i} å¸§ä½å§¿æ— æ•ˆï¼Œè·³è¿‡")
            continue

        # DA3 è¾“å‡ºä¸º c2w (camera-to-world), è½¬æ¢ä¸º OpenGL åæ ‡ç³»
        c2w_opengl = np.matmul(pose, flip_mat)

        frames_data.append(
            {
                "file_path": f"images/{dst_name}",
                "depth_file_path": f"depths/{dst_name}",
                "normal_file_path": f"normals_from_pretrain/{dst_name}",
                "transform_matrix": c2w_opengl.tolist(),
            }
        )

    # 5) å†™å…¥ transforms.json â€” ä½¿ç”¨å›¾ç‰‡çš„å®é™…åˆ†è¾¨ç‡
    output_json = {
        "fl_x": fx_scaled,
        "fl_y": fy_scaled,
        "cx": cx_scaled,
        "cy": cy_scaled,
        "w": int(img_w),
        "h": int(img_h),
        "k1": 0,
        "k2": 0,
        "p1": 0,
        "p2": 0,
        "camera_model": "OPENCV",
        "frames": frames_data,
    }

    json_path = dataset_dir / "transforms.json"
    with open(json_path, "w") as f:
        json.dump(output_json, f, indent=4)

    print(f"  âœ… æ•°æ®è½¬æ¢å®Œæˆ: {len(frames_data)} å¸§")
    print(f"     æ•°æ®é›†è·¯å¾„: {dataset_dir.resolve()}")
    return True


# ================= Step 2: DN-Splatter è®­ç»ƒ =================

def run_dn_splatter_training(dataset_dir: Path, output_dir: Path, experiment_name: str,
                             max_iterations: int = 30000):
    """
    ä½¿ç”¨ ns-train è¿è¡Œ DN-Splatter è®­ç»ƒ
    """
    print()
    print("=" * 60)
    print("ğŸ”¥ [Step 2] DN-Splatter è®­ç»ƒ")
    print("=" * 60)

    if not dataset_dir.exists():
        raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")

    output_dir.mkdir(parents=True, exist_ok=True)

    # ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

    cmd = [
        NS_PYTHON_EXE,
        NS_TRAIN,
        "dn-splatter",
        "--output-dir", str(output_dir),
        "--experiment-name", experiment_name,
        "--max-num-iterations", str(max_iterations),
        # DN-Splatter ç‰¹æœ‰çš„æ·±åº¦/æ³•çº¿æŸå¤±é…ç½®
        "--pipeline.model.use-depth-loss", "True",
        "--pipeline.model.depth-lambda", "0.2",
        "--pipeline.model.use-normal-loss", "True",
        "--pipeline.model.normal-lambda", "0.05",
        "--pipeline.model.predict-normals", "True",
        "--pipeline.model.use-normal-tv-loss", "True",
        "--pipeline.model.two-d-gaussians", "True",
        # ===== å¯†åº¦æ§åˆ¶ (RTX 5070 12GB ä¼˜åŒ–, ç›®æ ‡~10GBæ˜¾å­˜) =====
        "--pipeline.model.densify-grad-thresh", "0.0004",   # é€‚ä¸­çš„åˆ†è£‚é˜ˆå€¼
        "--pipeline.model.cull-alpha-thresh", "0.005",      # æ¸…ç†ä½é€æ˜åº¦é«˜æ–¯çƒ
        "--pipeline.model.stop-split-at", "12000",          # 12000æ­¥ååœæ­¢åˆ†è£‚
        "--pipeline.model.max-gs-num", "2000000",           # é«˜æ–¯çƒä¸Šé™ 200ä¸‡
        "--pipeline.model.sh-degree", "3",                  # SHé˜¶æ•°æ¢å¤ä¸º3, æ›´å¥½çš„é¢œè‰²è´¨é‡
        # ===== æ•°æ®åŠ è½½åŠ é€Ÿ =====
        "--pipeline.datamanager.dataloader-num-workers", "4", # å¤šçº¿ç¨‹åŠ è½½æ•°æ®, æå‡GPUåˆ©ç”¨ç‡
        # ===== Checkpoint ä¿å­˜ (æ¯5000æ­¥ä¿å­˜ä¸€æ¬¡, é˜²æ­¢ç¿»è½¦) =====
        "--steps-per-save", "5000",
        "--save-only-latest-checkpoint", "False",
        # Viewer è®¾ç½®
        "--viewer.websocket-port", "7007",
        "--viewer.quit-on-train-completion", "True",  # è®­ç»ƒå®Œè‡ªåŠ¨é€€å‡º, ä¸é˜»å¡
        "--vis", "viewer+tensorboard",
        # æ•°æ®è§£æå™¨ (normal-nerfstudio æ”¯æŒ json + depth + normal)
        "normal-nerfstudio",
        "--data", str(dataset_dir),
        # ä¸ä½¿ç”¨ç‚¹äº‘åˆå§‹åŒ– (DA3 è¾“å‡ºä¸­æ²¡æœ‰ SfM æ ¼å¼çš„ç‚¹äº‘)
        "--load-3D-points", "False",
        "--load-pcd-normals", "False",
    ]

    print(f"  ğŸ“‹ è®­ç»ƒå‘½ä»¤:\n  {' '.join(cmd[:5])} \\\n    {' '.join(cmd[5:])}")
    print()

    try:
        subprocess.run(cmd, check=True, env=env, cwd=str(PROJECT_ROOT))
        print("  âœ… è®­ç»ƒå®Œæˆ!")
        return True
    except subprocess.CalledProcessError as e:
        # Exit code 130 = SIGINT (Ctrl+C). å¦‚æœåœ¨ "Training Finished" åæŒ‰
        # Ctrl+C é€€å‡º viewer, è®­ç»ƒæœ¬èº«æ˜¯æˆåŠŸçš„, åº”è¯¥ç»§ç»­å¯¼å‡º PLY.
        if e.returncode == 130 or e.returncode == -2:
            print("  âš ï¸ è®­ç»ƒå®Œæˆä½†è¢« Ctrl+C ä¸­æ–­ (viewer é€€å‡º), ç»§ç»­å¯¼å‡º...")
            return True
        print(f"  âŒ è®­ç»ƒå¤±è´¥: {e}")
        return False


# ================= Step 3: å¯¼å‡º PLY =================

def export_ply(output_dir: Path, experiment_name: str):
    """
    è®­ç»ƒå®Œæˆåå¯¼å‡º Gaussian Splatting PLY æ–‡ä»¶
    """
    print()
    print("=" * 60)
    print("ğŸ“¤ [Step 3] å¯¼å‡º PLY")
    print("=" * 60)

    # æŸ¥æ‰¾æœ€æ–°çš„ config.yml
    config_paths = list((output_dir / experiment_name).rglob("config.yml"))
    if not config_paths:
        print("  âš ï¸  æœªå‘ç°è®­ç»ƒç”Ÿæˆçš„ config.ymlï¼Œæ— æ³•å¯¼å‡º PLY")
        return False

    latest_config = max(config_paths, key=lambda p: p.stat().st_mtime)
    export_dir = output_dir / "export"

    env = os.environ.copy()
    env["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

    cmd = [
        NS_PYTHON_EXE,
        NS_EXPORT,
        "gaussian-splat",
        "--load-config", str(latest_config),
        "--output-dir", str(export_dir),
    ]

    print(f"  ğŸ“‹ å¯¼å‡ºå‘½ä»¤: {' '.join(cmd)}")

    try:
        subprocess.run(cmd, check=True, env=env)
        print(f"  âœ… å¯¼å‡ºæˆåŠŸ! PLY æ–‡ä»¶: {export_dir}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"  âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return False


# ================= ğŸš€ ä¸»å…¥å£ =================

def main():
    parser = argparse.ArgumentParser(
        description="DA3 â†’ DN-Splatter ç»Ÿä¸€ Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤è·¯å¾„
  python run_da3_to_dn_splatter_pipeline.py

  # æŒ‡å®šæºç›®å½•å’Œè¾“å‡ºå
  python run_da3_to_dn_splatter_pipeline.py \\
    --source-dir /path/to/da3/output \\
    --output-name my_experiment

  # åªè·‘æ•°æ®è½¬æ¢ (è·³è¿‡è®­ç»ƒ)
  python run_da3_to_dn_splatter_pipeline.py --convert-only

  # è·³è¿‡æ•°æ®è½¬æ¢ (åªè®­ç»ƒ, å‡è®¾æ•°æ®é›†å·²å‡†å¤‡å¥½)
  python run_da3_to_dn_splatter_pipeline.py --train-only
        """,
    )
    parser.add_argument(
        "--source-dir", type=Path, default=DEFAULT_SOURCE_DIR,
        help="DA3 è¾“å‡ºç›®å½• (åŒ…å« extracted/, results_output/, intrinsic.txt, camera_poses.txt)",
    )
    parser.add_argument(
        "--output-name", type=str, default=DEFAULT_OUTPUT_NAME,
        help="å®éªŒåç§°ï¼Œç”¨äºæ•°æ®é›†ç›®å½•å’Œè®­ç»ƒè¾“å‡ºç›®å½•å‘½å",
    )
    parser.add_argument(
        "--max-iterations", type=int, default=30000,
        help="æœ€å¤§è®­ç»ƒè¿­ä»£æ•° (é»˜è®¤: 30000)",
    )
    parser.add_argument(
        "--convert-only", action="store_true",
        help="åªè¿è¡Œæ•°æ®æ ¼å¼è½¬æ¢ï¼Œä¸è®­ç»ƒ",
    )
    parser.add_argument(
        "--train-only", action="store_true",
        help="è·³è¿‡æ•°æ®è½¬æ¢ï¼Œç›´æ¥è®­ç»ƒ (å‡è®¾æ•°æ®é›†å·²å­˜åœ¨)",
    )
    parser.add_argument(
        "--skip-export", action="store_true",
        help="è®­ç»ƒå®Œåä¸å¯¼å‡º PLY",
    )
    parser.add_argument(
        "--clean", action="store_true",
        help="æ¸…é™¤å·²æœ‰æ•°æ®é›†ç›®å½•åé‡æ–°è½¬æ¢",
    )

    args = parser.parse_args()

    source_dir = args.source_dir
    dataset_dir = PROJECT_ROOT / f"{args.output_name}_dataset"
    output_dir = PROJECT_ROOT / f"{args.output_name}_output"
    experiment_name = args.output_name

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘       DA3 â†’ DN-Splatter ç»Ÿä¸€ Pipeline                   â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print(f"â•‘  æºç›®å½•:    {str(source_dir)[:45]:45s} â•‘")
    print(f"â•‘  æ•°æ®é›†:    {str(dataset_dir)[:45]:45s} â•‘")
    print(f"â•‘  è¾“å‡ºç›®å½•:  {str(output_dir)[:45]:45s} â•‘")
    print(f"â•‘  æœ€å¤§è¿­ä»£:  {args.max_iterations:<45d} â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    start_time = time.time()

    # Step 1: æ•°æ®è½¬æ¢
    if not args.train_only:
        if args.clean and dataset_dir.exists():
            print(f"ğŸ—‘ï¸  æ¸…é™¤å·²æœ‰æ•°æ®é›†: {dataset_dir}")
            shutil.rmtree(dataset_dir)

        if not convert_da3_to_dn_splatter(source_dir, dataset_dir):
            print("âŒ æ•°æ®è½¬æ¢å¤±è´¥ï¼Œä¸­æ­¢æµæ°´çº¿")
            sys.exit(1)

    if args.convert_only:
        elapsed = time.time() - start_time
        print(f"\nâ±ï¸  æ•°æ®è½¬æ¢è€—æ—¶: {elapsed:.1f}s")
        print("âœ… æ•°æ®è½¬æ¢å®Œæˆ (--convert-only æ¨¡å¼ï¼Œä¸è®­ç»ƒ)")
        return

    # Step 2: è®­ç»ƒ
    time.sleep(1)  # ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
    if not run_dn_splatter_training(dataset_dir, output_dir, experiment_name,
                                     args.max_iterations):
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œä¸­æ­¢æµæ°´çº¿")
        sys.exit(1)

    # Step 3: å¯¼å‡º
    if not args.skip_export:
        export_ply(output_dir, experiment_name)

    elapsed = time.time() - start_time
    print()
    print("=" * 60)
    print(f"ğŸ‰ Pipeline å®Œæˆ! æ€»è€—æ—¶: {elapsed:.1f}s ({elapsed/60:.1f}min)")
    print("=" * 60)


if __name__ == "__main__":
    main()
